{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from UnarySim.sw.kernel.linear_st import LinearST\n",
    "from UnarySim.sw.stream.gen import RNG, SourceGen, BSGen\n",
    "from UnarySim.sw.metric.metric import ProgressiveError\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummaryX import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from UnarySim.sw.kernel.nn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\project\\Anaconda3\\Lib\\site-packages\\UnarySim\\sw\\test\\mlp\n"
     ]
    }
   ],
   "source": [
    "bitwidth = 8\n",
    "# layer_width = 512\n",
    "# lr = 0.001\n",
    "\n",
    "# layer_width = 1024\n",
    "# lr = 0.001\n",
    "\n",
    "layer_width = 2048\n",
    "lr = 0.0001\n",
    "\n",
    "# layer_width = 4096\n",
    "# lr = 0.0001\n",
    "\n",
    "# layer_width = 16384\n",
    "# lr = 0.0001\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "model_path = cwd+\"\\saved_model_state_dict\"+\"_\"+str(bitwidth)+\"_bitwidth_\"+str(layer_width)+\"_layerwidth_\"+str(lr)+\"_lr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95.310000 %\n"
     ]
    }
   ],
   "source": [
    "# MNIST data loader\n",
    "transform=transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "datadir = 'd:/project/Anaconda3/Lib/site-packages/UnarySim/sw/test/mlp/data/mnist'\n",
    "trainset = torchvision.datasets.MNIST(root=datadir, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=datadir, train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=4)\n",
    "\n",
    "model = MLP3(layer_width)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images.max(), images.min())\n",
    "# print(\"model fc1 wght: max:\", model.fc1.weight.max().item(), \"min:\", model.fc1.weight.min().item())\n",
    "# print(\"model fc1 bias: max:\", model.fc1.bias.max().item(),   \"min:\", model.fc1.bias.min().item())\n",
    "# print(\"model fc2 wght: max:\", model.fc2.weight.max().item(), \"min:\", model.fc2.weight.min().item())\n",
    "# print(\"model fc2 bias: max:\", model.fc2.bias.max().item(),   \"min:\", model.fc2.bias.min().item())\n",
    "# print(\"model fc3 wght: max:\", model.fc3.weight.max().item(), \"min:\", model.fc3.weight.min().item())\n",
    "# print(\"model fc3 bias: max:\", model.fc3.bias.max().item(),   \"min:\", model.fc3.bias.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fc1_wght_freq = np.fft.fft2(model.fc1.weight.clone().detach().cpu().numpy())\n",
    "# print(fc1_wght_freq)\n",
    "# print(images.shape)\n",
    "# print(images[0][0].shape)\n",
    "# image_freq = np.fft.fft2(images[0][0].clone().detach().cpu().numpy() * 255)\n",
    "# print(image_freq)\n",
    "# print(images[0][0].clone().detach().cpu().numpy().max())\n",
    "# im_array = np.asarray(images[0][0].clone().detach().cpu().numpy() * 255)\n",
    "# plt.imshow(im_array, cmap='gray', vmin=0, vmax=255)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 48.000000 %\n"
     ]
    }
   ],
   "source": [
    "rng = \"Sobol\"\n",
    "rng_width = 8\n",
    "bias = True\n",
    "population = 1\n",
    "rng_stride = 3\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "total_cnt = 100\n",
    "\n",
    "fc1 = LinearST(32*32, layer_width, model.fc1.weight/model.fc1.weight.abs().max().item(), model.fc1.bias/model.fc1.weight.abs().max().item(), bias=bias, \n",
    "               mode=\"bipolar\", rng=rng, rng_width=rng_width, rng_stride=rng_stride, population=population).to(device)\n",
    "fc2 = LinearST(layer_width,   layer_width, model.fc2.weight/model.fc2.weight.abs().max().item(), model.fc2.bias/model.fc2.weight.abs().max().item(), bias=bias, \n",
    "               mode=\"bipolar\", rng=rng, rng_width=rng_width, rng_stride=rng_stride, population=population).to(device)\n",
    "fc3 = LinearST(layer_width,   10,  model.fc3.weight/model.fc3.weight.abs().max().item(), model.fc3.bias/model.fc3.weight.abs().max().item(), bias=bias, \n",
    "               mode=\"bipolar\", rng=rng, rng_width=rng_width, rng_stride=rng_stride, population=population).to(device)\n",
    "with torch.no_grad():\n",
    "    index = 0\n",
    "    for data in testloader:\n",
    "        index += 1\n",
    "        if index > total_cnt:\n",
    "            break\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        x = images.view(-1, 32*32)\n",
    "        fc1_out = fc1(x)\n",
    "#         print(fc1_out.max().item(), fc1_out.min().item())\n",
    "        fc1_scale = fc1_out.abs().max()\n",
    "        fc1_out = fc1_out / fc1_scale\n",
    "#         print(fc1_out.max().item(), fc1_out.min().item())\n",
    "        fc1_act = F.relu(fc1_out)\n",
    "\n",
    "        fc2_out = fc2(fc1_act)\n",
    "        fc2_scale = fc2_out.abs().max()\n",
    "        fc2_out = fc2_out / fc2_scale\n",
    "        fc2_act = F.relu(fc2_out)\n",
    "\n",
    "        fc3_out = fc3(fc2_act)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        fc1_ref_out = model.fc1_out\n",
    "        fc2_ref_out = model.fc2_out\n",
    "        fc3_ref_out = model.fc3_out\n",
    "        fc1_ref_act = model.relu1_out\n",
    "        fc2_ref_act = model.relu2_out\n",
    "#         print(fc1_ref_out)\n",
    "#         print(fc1_out)\n",
    "#         print(fc1_ref_out - fc1_out)\n",
    "#         print(fc2_ref_act)\n",
    "#         print(fc2_act)\n",
    "#         print(torch.sum(torch.gt(fc2_ref_act, 0).type(torch.float)))\n",
    "#         print(torch.sum(torch.gt(fc2_act, 0).type(torch.float)))\n",
    "        \n",
    "#         print(torch.max(outputs.data, 1)[1])\n",
    "#         print(torch.max(fc3_out.data, 1)[1])\n",
    "        \n",
    "        _, predicted = torch.max(fc3_out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 8.000000 %\n"
     ]
    }
   ],
   "source": [
    "model_fp = MLP3(layer_width)\n",
    "bitwidth = 2\n",
    "model_fp.eval()\n",
    "model_fp.to(device)\n",
    "model_fp.fc1.weight.data = model.fc1.weight.mul(2**bitwidth).round().div(2**bitwidth).clone().detach()\n",
    "model_fp.fc1.bias.data   = model.fc1.bias.mul(2**bitwidth).round().div(2**bitwidth).clone().detach()\n",
    "model_fp.fc2.weight.data = model.fc2.weight.mul(2**bitwidth).round().div(2**bitwidth).clone().detach()\n",
    "model_fp.fc2.bias.data   = model.fc2.bias.mul(2**bitwidth).round().div(2**bitwidth).clone().detach()\n",
    "model_fp.fc3.weight.data = model.fc3.weight.mul(2**bitwidth).round().div(2**bitwidth).clone().detach()\n",
    "model_fp.fc3.bias.data   = model.fc3.bias.mul(2**bitwidth).round().div(2**bitwidth).clone().detach()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "total_cnt = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    index = 0\n",
    "    for data in testloader:\n",
    "        index += 1\n",
    "        if index > total_cnt:\n",
    "            break\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model_fp(images.mul(2**bitwidth).round().div(2**bitwidth))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512\n",
    "\n",
    "population : fp mul acc : new design acc : xnornet acc : fp model acc : \n",
    "\n",
    "         1 :        13% :            13% :         24% :              : \n",
    "         2 :        11% :            14% :         24% :           8% : bw0\n",
    "         4 :        53% :            17% :         24% :          29% : bw1\n",
    "         8 :        87% :            21% :         24% :          98% : bw2\n",
    "        16 :        91% :            48% :         24% :          99% : bw3\n",
    "        32 :        97% :            46% :         24% :          99% : bw4\n",
    "        64 :        96% :            54% :         24% :          99% : bw5\n",
    "       128 :        97% :            80% :         24% :          99% : bw6\n",
    "       256 :        97% :            97% :         24% :          99% : bw7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1024\n",
    "\n",
    "population : fp mul acc : new design acc : xnornet acc : fp model acc : \n",
    "\n",
    "         1 :         8% :             8% :         27% :              : \n",
    "         2 :        18% :            16% :         27% :           8% : bw0\n",
    "         4 :        88% :            18% :         27% :          93% : bw1\n",
    "         8 :        96% :            14% :         27% :          98% : bw2\n",
    "        16 :        98% :            70% :         27% :          98% : bw3\n",
    "        32 :        98% :            70% :         27% :          98% : bw4\n",
    "        64 :        98% :            98% :         27% :          98% : bw5\n",
    "       128 :        98% :            98% :         27% :          98% : bw6\n",
    "       256 :        98% :            98% :         27% :          98% : bw7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2048\n",
    "\n",
    "population : fp mul acc : new design acc : xnornet acc : fp model acc : \n",
    "\n",
    "         1 :         9% :             9% :         48% :              : \n",
    "         2 :        14% :            15% :         48% :           8% : bw0\n",
    "         4 :        84% :            28% :         48% :           8% : bw1\n",
    "         8 :        96% :            32% :         48% :           8% : bw2\n",
    "        16 :        98% :            71% :         48% :          24% : bw3\n",
    "        32 :        98% :            86% :         48% :          96% : bw4\n",
    "        64 :        98% :            98% :         48% :          98% : bw5\n",
    "       128 :        98% :            98% :         48% :          98% : bw6\n",
    "       256 :        98% :            98% :         48% :          98% : bw7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
