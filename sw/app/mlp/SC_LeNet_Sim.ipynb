{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummaryX import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\project\\Anaconda3\\Lib\\site-packages\\UnarySim\\sw\\test\n",
      "D:\\project\\Anaconda3\\Lib\\site-packages\\UnarySim\\sw\\test/saved_model_state_dict_8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "# model_path = cwd+\"/saved_model_state_dict_bw_8test\"\n",
    "model_path = cwd+\"/saved_model_state_dict_8\"\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data loader\n",
    "transform=transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data/mnist', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc1_drop): Dropout(p=0.6, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2_drop): Dropout(p=0.6, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = self.conv1(x)\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        x = F.avg_pool2d(x, (2, 2))\n",
    "        x = F.relu(x)\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = self.conv2(x)\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        x = F.avg_pool2d(x, (2, 2))\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 512)\n",
    "        self.fc1_drop = nn.Dropout(0.6)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc2_drop = nn.Dropout(0.6)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)        \n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "    \n",
    "model = Net()\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0067,  0.0076, -0.0124,  ...,  0.0016, -0.0136, -0.0302],\n",
       "        [-0.0240, -0.0231, -0.0116,  ..., -0.0007,  0.0182, -0.0108],\n",
       "        [-0.0158,  0.0176,  0.0058,  ..., -0.0142, -0.0004, -0.0268],\n",
       "        ...,\n",
       "        [-0.0035,  0.0192,  0.0305,  ...,  0.0065, -0.0179, -0.0098],\n",
       "        [-0.0148,  0.0187, -0.0257,  ...,  0.0257, -0.0072, -0.0086],\n",
       "        [-0.0175, -0.0224, -0.0101,  ..., -0.0285,  0.0044, -0.0090]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc1_drop): Dropout(p=0.6, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2_drop): Dropout(p=0.6, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "Accuracy of the network on the 10000 test images: 94.550000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(images.size())\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitStreamGen(object):\n",
    "    def __init__(self, input, bitwidth=8, bipolar=True, dim=1, mode=\"Sobol\"):\n",
    "        super(BitStreamGen, self).__init__()\n",
    "        self.input = input\n",
    "        self.bitwidth = bitwidth\n",
    "        self.bipolar = bipolar\n",
    "        self.index = 0\n",
    "        self.dim = dim\n",
    "        self.mode = mode\n",
    "        self.seq_len = pow(2,self.bitwidth)\n",
    "        self.out = 0.0\n",
    "        # random_sequence from sobol RNG\n",
    "        if self.mode == \"Sobol\":\n",
    "            self.random_sequence = torch.quasirandom.SobolEngine(self.dim).draw(self.seq_len).view(self.seq_len)\n",
    "        elif self.mode == \"Race\":\n",
    "            self.random_sequence = torch.tensor([x/self.seq_len for x in range(self.seq_len)])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if self.bipolar is True:\n",
    "            # convert to bipolar\n",
    "            self.random_sequence.mul_(2).sub_(1)\n",
    "    \n",
    "    def Gen(self):\n",
    "        self.out = torch.gt(self.input, self.random_sequence[self.index]).type(torch.float)\n",
    "        self.index += 1\n",
    "        return self.out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressivePrecision(object):\n",
    "    def __init__(self, actual_value, bitwidth=8, bipolar=True, auto_print=False):\n",
    "        super(ProgressivePrecision, self).__init__()\n",
    "        self.actual_value = actual_value\n",
    "        self.bitwidth = bitwidth\n",
    "        self.bipolar = bipolar\n",
    "        self.index = 0.0\n",
    "        self.one_cnt = 0.0\n",
    "        self.seq_len = pow(2,self.bitwidth)\n",
    "        self.out_pp = 0.0\n",
    "        self.error = 0.0\n",
    "        self.auto_print = auto_print\n",
    "        self.pp_list = []\n",
    "    \n",
    "    def Monitor(self, input):\n",
    "        self.one_cnt += input\n",
    "        self.index += 1\n",
    "        self.out_pp = self.one_cnt / self.index\n",
    "        if self.bipolar is True:\n",
    "            self.out_pp = 2 * self.out_pp - 1\n",
    "        self.pp_list.append(self.out_pp)\n",
    "        \n",
    "        self.err = self.out_pp - self.actual_value\n",
    "        self.err_abs = self.err.abs()\n",
    "        if self.auto_print is True:\n",
    "            print(\"Progressive Error:\", self.err)\n",
    "        if self.index == self.seq_len:\n",
    "            print(\"Final Error:\", self.err)\n",
    "            print(\"Final Value:\", self.out_pp)\n",
    "\n",
    "        return self.err\n",
    "    \n",
    "    def Stability(self, threshold):\n",
    "        if self.index is self.seq_len:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class UnaryConv2d(torch.nn.modules.conv.Conv2d):\n",
    "    \"\"\"This is bipolar mul and non-scaled addition\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, output_shape,\n",
    "                 binary_weight=torch.tensor([0]), binary_bias=torch.tensor([0]), bitwidth=8, \n",
    "                 stride=1, padding=0, dilation=1, \n",
    "                 groups=1, bias=True, padding_mode='zeros'):\n",
    "        super(UnaryConv2d, self).__init__(in_channels, out_channels, kernel_size)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # data bit width\n",
    "        self.buf_wght = binary_weight.clone().detach()\n",
    "        if bias is True:\n",
    "            self.buf_bias = binary_bias.clone().detach()\n",
    "        self.bitwidth = bitwidth\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        self.groups = groups\n",
    "        self.has_bias = bias\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "        # random_sequence from sobol RNG\n",
    "        self.rng = torch.quasirandom.SobolEngine(1).draw(pow(2,self.bitwidth)).view(pow(2,self.bitwidth))\n",
    "        # convert to bipolar\n",
    "        self.rng.mul_(2).sub_(1)\n",
    "#         print(self.rng)\n",
    "\n",
    "        # define the kernel linear\n",
    "        self.kernel = torch.nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, \n",
    "                              stride=self.stride, padding=self.padding, dilation=self.dilation, \n",
    "                              groups=self.groups, bias=self.has_bias, padding_mode=self.padding_mode)\n",
    "\n",
    "        # define the RNG index tensor for weight\n",
    "        self.rng_wght_idx = torch.zeros(self.kernel.weight.size(), dtype=torch.long)\n",
    "        self.rng_wght = self.rng[self.rng_wght_idx]\n",
    "        assert (self.buf_wght.size() == self.rng_wght.size()\n",
    "               ), \"Input binary weight size of 'kernel' is different from true weight.\"\n",
    "        \n",
    "        # define the RNG index tensor for bias if available, only one is required for accumulation\n",
    "        if self.has_bias is True:\n",
    "            print(\"Has bias.\")\n",
    "            self.rng_bias_idx = torch.zeros(self.kernel.bias.size(), dtype=torch.long)\n",
    "            self.rng_bias = self.rng[self.rng_bias_idx]\n",
    "            assert (self.buf_bias.size() == self.rng_bias.size()\n",
    "                   ), \"Input binary bias size of 'kernel' is different from true bias.\"\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        # inverse\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        # define the kernel_inverse, no bias required\n",
    "        self.kernel_inv = torch.nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, \n",
    "                              stride=self.stride, padding=self.padding, dilation=self.dilation, \n",
    "                              groups=self.groups, bias=False, padding_mode=self.padding_mode)\n",
    "        \n",
    "        # define the RNG index tensor for weight_inverse\n",
    "        self.rng_wght_idx_inv = torch.zeros(self.kernel_inv.weight.size(), dtype=torch.long)\n",
    "        self.rng_wght_inv = self.rng[self.rng_wght_idx_inv]\n",
    "        assert (self.buf_wght.size() == self.rng_wght_inv.size()\n",
    "               ), \"Input binary weight size of 'kernel_inv' is different from true weight.\"\n",
    "        \n",
    "        self.in_accumulator = torch.zeros(output_shape)\n",
    "        self.out_accumulator = torch.zeros(output_shape)\n",
    "        self.output = torch.zeros(output_shape)\n",
    "    \n",
    "    def UnaryKernel_nonscaled_forward(self, input):\n",
    "        # generate weight bits for current cycle\n",
    "        self.rng_wght = self.rng[self.rng_wght_idx]\n",
    "        self.kernel.weight.data = torch.gt(self.buf_wght, self.rng_wght).type(torch.float)\n",
    "        print(self.rng_wght_idx.size())\n",
    "        print(input.size())\n",
    "        self.rng_wght_idx.add_(input.type(torch.long))\n",
    "        if self.has_bias is True:\n",
    "            self.rng_bias = self.rng[self.rng_bias_idx]\n",
    "            self.kernel.bias.data = torch.gt(self.buf_bias, self.rng_bias).type(torch.float)\n",
    "            self.rng_bias_idx.add_(1)\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        # inverse\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        self.rng_wght_inv = self.rng[self.rng_wght_idx_inv].type(torch.float)\n",
    "        self.kernel_inv.weight.data = torch.le(self.buf_wght, self.rng_wght_inv).type(torch.float)\n",
    "        self.rng_wght_idx_inv.add_(1).sub_(input.type(torch.long))\n",
    "#         print(self.kernel(input).size())\n",
    "        return self.kernel(input) + self.kernel_inv(1-input)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.in_accumulator.add_(self.UnaryKernel_nonscaled_forward(input))\n",
    "#         .clamp_(-self.upper_bound, self.upper_bound)\n",
    "        self.in_accumulator.sub_(self.offset)\n",
    "        self.output = torch.gt(self.in_accumulator, self.out_accumulator).type(torch.float)\n",
    "#         print(\"accumulator result:\", self.in_accumulator, self.out_accumulator)\n",
    "        self.out_accumulator.add_(self.output)\n",
    "        return self.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv = torch.nn.Conv2d(1,6,5)\n",
    "# print(conv.weight.size())\n",
    "# print(conv.bias.size())\n",
    "# conv.weight.data = torch.rand(conv.weight.size()) * 2 -1\n",
    "# conv.bias.data = torch.rand(conv.bias.size()) * 2 -1\n",
    "\n",
    "# uconv = UnaryConv2d(1, 6, 5, (1, 6, 28, 28), conv.weight, conv.bias)\n",
    "\n",
    "# inVec = torch.rand(1,1,32,32).floor()/256\n",
    "# # print(inVec)\n",
    "# outVec = conv(inVec)\n",
    "# outVec.clamp_(-1.,1.)\n",
    "\n",
    "# bsGen = BitStreamGen(inVec, mode=\"Sobol\", bipolar=True)\n",
    "# # bsGen = BitStreamGen(inVec,mode=\"Race\")\n",
    "# ipp = ProgressivePrecision(inVec, auto_print=False,bipolar=True)\n",
    "# pp = ProgressivePrecision(outVec, auto_print=False,bipolar=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(256):\n",
    "#         input = bsGen.Gen()\n",
    "#         output = uconv(input)\n",
    "#         pp.Monitor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a(torch.zeros(1,1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class UnaryLinear(torch.nn.modules.linear.Linear):\n",
    "    def __init__(self, in_features, out_features, upper_bound,\n",
    "                 binary_weight=torch.tensor([0]), binary_bias=torch.tensor([0]), bitwidth=8,\n",
    "                 bias=True):\n",
    "        super(UnaryLinear, self).__init__(in_features, out_features)\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.upper_bound = upper_bound\n",
    "        # bipolar accumulation\n",
    "        self.offset = (in_features-1)/2\n",
    "        \n",
    "        # data bit width\n",
    "        self.buf_wght = binary_weight.clone().detach()\n",
    "        if bias is True:\n",
    "            self.buf_bias = binary_bias.clone().detach()\n",
    "        self.bitwidth = bitwidth\n",
    "        \n",
    "        self.has_bias = bias\n",
    "        \n",
    "        # random_sequence from sobol RNG\n",
    "        self.rng = torch.quasirandom.SobolEngine(1).draw(pow(2,self.bitwidth)).view(pow(2,self.bitwidth))\n",
    "        # convert to bipolar\n",
    "        self.rng.mul_(2).sub_(1)\n",
    "#         print(self.rng)\n",
    "\n",
    "        # define the kernel linear\n",
    "        self.kernel = torch.nn.Linear(self.in_features, self.out_features,\n",
    "                                  bias=self.has_bias)\n",
    "\n",
    "        # define the RNG index tensor for weight\n",
    "        self.rng_wght_idx = torch.zeros(self.kernel.weight.size(), dtype=torch.long)\n",
    "        self.rng_wght = self.rng[self.rng_wght_idx]\n",
    "        assert (self.buf_wght.size() == self.rng_wght.size()\n",
    "               ), \"Input binary weight size of 'kernel' is different from true weight.\"\n",
    "        \n",
    "        # define the RNG index tensor for bias if available, only one is required for accumulation\n",
    "        if self.has_bias is True:\n",
    "            print(\"Has bias.\")\n",
    "            self.rng_bias_idx = torch.zeros(self.kernel.bias.size(), dtype=torch.long)\n",
    "            self.rng_bias = self.rng[self.rng_bias_idx]\n",
    "            assert (self.buf_bias.size() == self.rng_bias.size()\n",
    "                   ), \"Input binary bias size of 'kernel' is different from true bias.\"\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        # inverse\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        # define the kernel_inverse, no bias required\n",
    "        self.kernel_inv = torch.nn.Linear(self.in_features, self.out_features,\n",
    "                                  bias=False)\n",
    "        \n",
    "        # define the RNG index tensor for weight_inverse\n",
    "        self.rng_wght_idx_inv = torch.zeros(self.kernel_inv.weight.size(), dtype=torch.long)\n",
    "        self.rng_wght_inv = self.rng[self.rng_wght_idx_inv]\n",
    "        assert (self.buf_wght.size() == self.rng_wght_inv.size()\n",
    "               ), \"Input binary weight size of 'kernel_inv' is different from true weight.\"\n",
    "        \n",
    "        self.in_accumulator = torch.zeros([1,out_features])\n",
    "        self.out_accumulator = torch.zeros([1,out_features])\n",
    "        self.output = torch.zeros([1,out_features])\n",
    "#         self.cycle = 0\n",
    "\n",
    "    def UnaryKernel_nonscaled_forward(self, input):\n",
    "        # generate weight bits for current cycle\n",
    "        self.rng_wght = self.rng[self.rng_wght_idx]\n",
    "        self.kernel.weight.data = torch.gt(self.buf_wght, self.rng_wght).type(torch.float)\n",
    "        self.rng_wght_idx.add_(input.type(torch.long))\n",
    "        if self.has_bias is True:\n",
    "            self.rng_bias = self.rng[self.rng_bias_idx]\n",
    "            self.kernel.bias.data = torch.gt(self.buf_bias, self.rng_bias).type(torch.float)\n",
    "            self.rng_bias_idx.add_(1)\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        # inverse\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        self.rng_wght_inv = self.rng[self.rng_wght_idx_inv].type(torch.float)\n",
    "        self.kernel_inv.weight.data = torch.le(self.buf_wght, self.rng_wght_inv).type(torch.float)\n",
    "        self.rng_wght_idx_inv.add_(1).sub_(input.type(torch.long))\n",
    "        return self.kernel(input) + self.kernel_inv(1-input)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.in_accumulator.add_(self.UnaryKernel_nonscaled_forward(input))\n",
    "#         .clamp_(-self.upper_bound, self.upper_bound)\n",
    "        self.in_accumulator.sub_(self.offset)\n",
    "        self.output = torch.gt(self.in_accumulator, self.out_accumulator).type(torch.float)\n",
    "#         print(\"accumulator result:\", self.in_accumulator, self.out_accumulator)\n",
    "        self.out_accumulator.add_(self.output)\n",
    "        return self.output\n",
    "#         return self.UnaryKernel_nonscaled_forward(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1024])\n",
      "torch.Size([512, 1024])\n",
      "Has bias.\n",
      "Final Error: tensor([[ 0.5566, -0.0234,  0.0625,  0.0859,  0.1406,  0.9650, -0.0614, -0.0547,\n",
      "         -0.0781,  0.0000, -0.0156,  0.4093,  0.7317,  0.1016,  0.5703,  0.8843,\n",
      "         -0.1094,  0.0547,  0.0781,  0.1172,  0.8516,  0.1875,  0.9952, -0.0469,\n",
      "          0.7385,  0.1614,  0.1172,  0.4652, -0.1016, -0.0078,  0.0547,  0.1562,\n",
      "         -0.0156,  0.9602,  0.5469, -0.0078,  0.8688,  0.0859,  0.0469,  0.1172,\n",
      "          0.9531, -0.0312, -0.0469,  0.9720, -0.0391, -0.0391, -0.0312,  0.7656,\n",
      "          0.0547,  0.0000,  0.1797, -0.0078,  0.1328,  0.1459,  0.0703,  0.7105,\n",
      "          0.9622,  0.1016,  0.3206, -0.0469,  0.9068,  0.0312, -0.0078,  0.0000,\n",
      "         -0.0703, -0.0391,  0.1016, -0.0312,  0.1016, -0.1016,  0.6719,  0.6415,\n",
      "         -0.0078,  0.9711,  0.2609,  0.6641,  0.9683, -0.0312,  0.0000,  0.0547,\n",
      "          0.8125, -0.0391,  0.0000,  0.9974, -0.0156,  0.0859, -0.0781,  0.9609,\n",
      "         -0.0156,  0.8401,  0.0703,  0.5360,  0.6361, -0.0547,  0.0781,  0.1484,\n",
      "          0.0000,  0.7184, -0.0547, -0.0156, -0.0625,  0.0000,  0.4695,  0.0547,\n",
      "          0.7031,  0.0391,  0.8826,  0.7900,  0.5703,  0.5553,  0.3680,  0.9453,\n",
      "          0.9709,  0.6401,  0.0938,  0.2188, -0.0446,  0.7554,  0.0547,  0.0859,\n",
      "          0.3210,  0.4075,  0.0000,  0.6657,  0.1953,  0.1562, -0.0703,  0.1172,\n",
      "         -0.0547,  0.1406, -0.0156,  0.6089,  0.3125,  0.1562, -0.0234, -0.0078,\n",
      "          0.1172,  0.0547,  0.9655,  0.0000, -0.0156,  0.1406,  0.1172,  0.5469,\n",
      "          0.4207, -0.0156,  0.0312,  0.8125,  0.6847,  0.2109,  0.0859,  0.0000,\n",
      "          0.1328,  0.9324, -0.1016,  0.7422,  0.4966,  0.2188,  0.5625,  0.0677,\n",
      "         -0.0078,  0.7891,  0.0000,  0.2344,  0.0000,  0.0547,  0.0781,  0.1172,\n",
      "          0.0625,  0.6797,  0.8291,  0.3359,  0.1782, -0.0234, -0.0391,  0.2734,\n",
      "          0.0000, -0.0236, -0.0547, -0.0078,  0.2188, -0.0234,  0.6875,  0.1172,\n",
      "         -0.0625,  0.0703, -0.0625,  0.6164,  0.0000,  0.8047,  0.2969,  0.0418,\n",
      "         -0.0312,  0.7578,  0.7969,  0.1016,  0.8516,  0.2330,  0.4453, -0.1016,\n",
      "          0.0625, -0.0312,  0.0781,  0.1016, -0.0781,  0.8516,  0.6818, -0.0312,\n",
      "          0.4940,  0.0703, -0.0234,  0.0547,  0.7025,  1.1875,  0.0625,  0.5681,\n",
      "         -0.0781,  0.6905,  0.0781,  1.0587,  0.0547, -0.0703,  0.3733,  0.0000,\n",
      "          0.0469,  0.1797,  0.6313,  0.6953,  0.0958, -0.0078,  0.5859,  0.8281,\n",
      "          0.1250,  1.0312,  0.1537, -0.0859, -0.0078,  0.0781,  0.4219,  0.8463,\n",
      "          0.1250,  0.3906,  0.1338,  0.5391,  0.7608,  0.7656, -0.0943, -0.0703,\n",
      "          0.7500, -0.0620,  0.4531, -0.0547, -0.0234,  0.9643,  0.9874,  0.0391,\n",
      "          0.6484,  0.2188,  0.0547, -0.0312,  0.0000,  0.0938,  0.3750, -0.1094,\n",
      "         -0.0078,  0.0469, -0.0469,  0.6016,  0.2232, -0.1094, -0.0859, -0.0391,\n",
      "          0.7656,  0.3558,  0.0000, -0.0391,  0.2969, -0.0234,  0.5078,  0.2422,\n",
      "         -0.0078,  0.8906,  0.0000,  0.1016,  0.2109, -0.0312,  0.8672, -0.1094,\n",
      "         -0.0156,  0.0000,  0.6019,  0.7328,  0.0000,  1.0785,  0.1555,  0.7734,\n",
      "          0.1406,  0.1406,  0.7188,  0.8750,  0.0625,  0.8438,  0.0547,  0.9201,\n",
      "          0.1484,  0.0625, -0.0078,  0.0859,  0.1674,  0.1016,  0.0703,  0.1562,\n",
      "          0.7406,  0.0781,  0.6406,  0.0703,  0.5021,  0.8594, -0.0703,  0.0938,\n",
      "         -0.0156, -0.0234,  0.0000,  0.3305,  0.7467,  0.0000,  0.0703,  0.2266,\n",
      "          0.0312,  0.6430,  0.1234, -0.0469,  0.0625,  0.7578,  0.0000, -0.0547,\n",
      "          0.0000,  0.0000,  0.1484,  0.7628,  0.0547,  0.4253,  0.0000,  0.0547,\n",
      "          0.0859, -0.0312,  0.7812, -0.0781,  0.9354,  0.0000, -0.0156,  0.7671,\n",
      "          0.0000,  0.7500,  0.3939, -0.0312,  0.5312,  0.7825, -0.0469,  0.1953,\n",
      "         -0.0859,  0.0000,  0.5703,  0.2812,  0.8594,  0.8438,  0.3125,  0.5884,\n",
      "          0.6406,  0.1797, -0.0078,  0.1797,  0.0938,  0.0247, -0.0625,  0.8669,\n",
      "         -0.0312, -0.0156,  0.5781,  0.6641,  0.1406,  0.1953,  0.6255,  0.0000,\n",
      "          0.7051,  0.1172,  1.1227,  0.8786,  0.2925,  0.5720,  0.7422,  0.0703,\n",
      "          0.7179,  0.1094,  0.0660,  0.0000,  0.4609,  0.0000,  0.9919,  0.4688,\n",
      "          0.1756,  0.9594,  0.0938, -0.1172,  0.7264, -0.0234, -0.0234,  0.8297,\n",
      "          0.0000,  0.1016, -0.0312,  0.0781,  0.0000,  0.9755, -0.0078,  0.0703,\n",
      "          0.8125,  0.5769,  0.0000,  0.0469, -0.0391,  0.0781, -0.0625,  0.0000,\n",
      "          0.0000, -0.0541,  0.0547,  0.8516,  0.8828,  0.9219,  0.7188,  0.0703,\n",
      "          0.0000,  0.0859, -0.0469,  0.3516,  0.3672,  0.0547,  0.0938, -0.0703,\n",
      "         -0.0938, -0.0547,  0.8683,  0.6094,  0.9375,  0.7656,  0.1328,  0.1328,\n",
      "         -0.0109,  0.3203,  0.4452,  0.9031,  0.0000, -0.0234,  0.2656,  0.3828,\n",
      "          0.0547,  0.1562, -0.0547, -0.1094, -0.0312,  0.8585,  0.4384,  0.0781,\n",
      "          0.0859,  0.1953,  0.0000,  0.3203,  0.0469,  0.1172, -0.0078,  0.1153,\n",
      "          0.0000,  0.2898, -0.0781,  0.3187,  0.0938,  0.5469,  0.4609,  0.1875,\n",
      "         -0.0781,  0.2500,  0.5156,  0.0703,  0.5641, -0.0547, -0.0312,  0.0859,\n",
      "         -0.0078,  0.2109,  0.9083, -0.0078,  0.5156,  0.4821, -0.0781,  0.0000,\n",
      "          0.2109,  0.0000,  0.1094,  0.4766,  0.0000,  0.1953,  0.9375,  0.0000,\n",
      "         -0.0234,  0.9656,  0.0547, -0.0625,  0.8200,  0.0547, -0.1328,  0.8448]])\n",
      "Final Value: tensor([[ 0.9062,  0.9766, -0.9375, -0.9141, -0.8594, -0.0078,  0.8984,  0.9453,\n",
      "          0.9219,  1.0000,  0.9844,  0.6094,  0.6719, -0.8984, -0.4297,  0.6172,\n",
      "          0.8906, -0.9453, -0.9219, -0.8828, -0.1484, -0.8125,  0.0625,  0.9531,\n",
      "          0.3594,  0.8672, -0.8828,  0.8438,  0.8984,  0.9922, -0.9453, -0.8438,\n",
      "          0.9844,  0.2422, -0.4531,  0.9922,  0.4531, -0.9141, -0.9531, -0.8828,\n",
      "         -0.0469,  0.9688,  0.9531,  0.1094,  0.9609,  0.9609,  0.9688, -0.2344,\n",
      "         -0.9453,  1.0000, -0.8203,  0.9922, -0.8672,  0.8906, -0.9297,  0.2266,\n",
      "          0.2188, -0.8984,  0.8281,  0.9531,  0.3672, -0.9688,  0.9922,  1.0000,\n",
      "          0.9297,  0.9609, -0.8984,  0.9688, -0.8984,  0.8984, -0.3281,  0.8203,\n",
      "          0.9922,  0.2891,  0.8984, -0.3359, -0.0078,  0.9688,  1.0000, -0.9453,\n",
      "         -0.1875,  0.9609,  1.0000,  0.2031,  0.9844, -0.9141,  0.9219, -0.0391,\n",
      "          0.9844, -0.1016, -0.9297,  0.7656,  0.6875,  0.9453, -0.9219, -0.8516,\n",
      "          1.0000,  0.3984,  0.9453,  0.9844,  0.9375,  1.0000,  0.7422, -0.9453,\n",
      "         -0.2969, -0.9609,  0.4922, -0.0391, -0.4297,  0.6875,  0.8047, -0.0547,\n",
      "          0.2266,  0.7266, -0.9062, -0.7812,  0.8984,  0.7891, -0.9453, -0.9141,\n",
      "          0.8281,  0.9141,  1.0000,  0.3203, -0.8047, -0.8438,  0.9297, -0.8828,\n",
      "          0.9453, -0.8594,  0.9844,  0.5234, -0.6875, -0.8438,  0.9766,  0.9922,\n",
      "         -0.8828, -0.9453,  0.4062,  1.0000,  0.9844, -0.8594, -0.8828, -0.4531,\n",
      "          0.6797,  0.9844, -0.9688, -0.1875,  0.5312, -0.7891, -0.9141,  1.0000,\n",
      "         -0.8672, -0.0312,  0.8984, -0.2578,  0.7266, -0.7812, -0.4375,  0.8047,\n",
      "          0.9922, -0.2109,  1.0000, -0.7656,  1.0000, -0.9453, -0.9219, -0.8828,\n",
      "         -0.9375, -0.3203,  0.1406, -0.6641,  0.8906,  0.9766,  0.9609, -0.7266,\n",
      "          1.0000,  0.9141,  0.9453,  0.9922, -0.7812,  0.9766, -0.3125, -0.8828,\n",
      "          0.9375, -0.9297,  0.9375,  0.8125,  1.0000, -0.1953, -0.7031,  0.9531,\n",
      "          0.9688, -0.2422, -0.2031, -0.8984, -0.1484,  0.8125, -0.5547,  0.8984,\n",
      "         -0.9375,  0.9688, -0.9219, -0.8984,  0.9219, -0.1484,  0.4922,  0.9688,\n",
      "          0.7188, -0.9297,  0.9766, -0.9453,  0.6328,  0.1875, -0.9375,  0.9141,\n",
      "          0.9219,  0.0078, -0.9219,  0.1484, -0.9453,  0.9297,  0.7422,  1.0000,\n",
      "         -0.9531, -0.8203,  0.4609, -0.3047,  0.7500,  0.9922, -0.4141, -0.1719,\n",
      "         -0.8750,  0.0312,  0.8984,  0.9141,  0.9922, -0.9219, -0.5781, -0.1328,\n",
      "         -0.8750, -0.6094,  0.9141, -0.4609,  0.6016, -0.2344,  0.8516,  0.9297,\n",
      "         -0.2500,  0.9062, -0.5469,  0.9453,  0.9766,  0.2734,  0.1797, -0.9609,\n",
      "         -0.3516, -0.7812, -0.9453,  0.9688,  1.0000, -0.9062, -0.6250,  0.8906,\n",
      "          0.9922, -0.9531,  0.9531, -0.3984,  0.8906,  0.8906,  0.9141,  0.9609,\n",
      "         -0.2344,  0.8359,  1.0000,  0.9609, -0.7031,  0.9766, -0.4922, -0.7578,\n",
      "          0.9922, -0.1094,  1.0000, -0.8984, -0.7891,  0.9688, -0.1328,  0.8906,\n",
      "          0.9844,  1.0000,  0.8594,  0.5625,  1.0000,  0.1172,  0.9375, -0.2266,\n",
      "         -0.8594, -0.8594, -0.2812, -0.1250, -0.9375, -0.1562, -0.9453,  0.1406,\n",
      "         -0.8516, -0.9375,  0.9922, -0.9141,  0.8750, -0.8984, -0.9297, -0.8438,\n",
      "          0.1953, -0.9219, -0.3594, -0.9297,  0.7656, -0.1406,  0.9297, -0.9062,\n",
      "          0.9844,  0.9766,  1.0000,  0.8828,  0.6719,  1.0000, -0.9297, -0.7734,\n",
      "         -0.9688,  0.4297,  0.9219,  0.9531, -0.9375, -0.2422,  1.0000,  0.9453,\n",
      "          1.0000,  1.0000, -0.8516,  0.5156, -0.9453,  0.8281,  1.0000, -0.9453,\n",
      "         -0.9141,  0.9688, -0.2188,  0.9219,  0.5469,  1.0000,  0.9844,  0.2500,\n",
      "          1.0000, -0.2500,  0.8281,  0.9688, -0.4688,  0.2500,  0.9531, -0.8047,\n",
      "          0.9141,  1.0000, -0.4297, -0.7188, -0.1406, -0.1562, -0.6875,  0.7578,\n",
      "         -0.3594, -0.8203,  0.9922, -0.8203, -0.9062,  0.9297,  0.9375,  0.6562,\n",
      "          0.9688,  0.9844, -0.4219, -0.3359, -0.8594, -0.8047,  0.2969,  1.0000,\n",
      "          0.0000, -0.8828,  0.2812,  0.2031,  0.8281,  0.3594, -0.2578, -0.9297,\n",
      "          0.4375, -0.8906,  0.8203,  1.0000, -0.5391,  1.0000,  0.4297, -0.5312,\n",
      "          0.9062, -0.0312, -0.9062,  0.8828,  0.7109,  0.9766,  0.9766,  0.5469,\n",
      "          1.0000, -0.8984,  0.9688, -0.9219,  1.0000,  0.5859,  0.9922, -0.9297,\n",
      "         -0.1875,  0.0547,  1.0000, -0.9531,  0.9609, -0.9219,  0.9375,  1.0000,\n",
      "          1.0000,  0.9062, -0.9453, -0.1484, -0.1172, -0.0781, -0.2812, -0.9297,\n",
      "          1.0000, -0.9141,  0.9531, -0.6484, -0.6328, -0.9453, -0.9062,  0.9297,\n",
      "          0.9062,  0.9453,  0.4688, -0.3906, -0.0625, -0.2344, -0.8672, -0.8672,\n",
      "          0.8672, -0.6797,  0.7656,  0.1250,  1.0000,  0.9766, -0.7344, -0.6172,\n",
      "         -0.9453, -0.8438,  0.9453,  0.8906,  0.9688,  0.5703,  0.7969, -0.9219,\n",
      "         -0.9141, -0.8047,  1.0000, -0.6797, -0.9531, -0.8828,  0.9922,  0.9453,\n",
      "          1.0000,  0.7734,  0.9219,  0.8750, -0.9062, -0.4531, -0.5391, -0.8125,\n",
      "          0.9219, -0.7500, -0.4844, -0.9297,  0.7500,  0.9453,  0.9688, -0.9141,\n",
      "          0.9922, -0.7891,  0.1719,  0.9922, -0.4844,  0.8516,  0.9219,  1.0000,\n",
      "         -0.7891,  1.0000, -0.8906, -0.5234,  1.0000, -0.8047, -0.0625,  1.0000,\n",
      "          0.9766,  0.1484, -0.9453,  0.9375,  0.0547, -0.9453,  0.8672,  0.5859]])\n"
     ]
    }
   ],
   "source": [
    "# fc400 = nn.Linear(400, 120)\n",
    "# fc400.weight = lenet.fc1.weight\n",
    "# fc400.bias = lenet.fc1.bias\n",
    "# ufc400 = UnaryLinear(400, 120, 256, lenet.fc1.weight, lenet.fc1.bias)\n",
    "fc400 = nn.Linear(1024, 512, bias=True)\n",
    "print(fc400.weight.size())\n",
    "fc400.weight.data = model.fc1.weight\n",
    "fc400.bias.data = model.fc1.bias\n",
    "# fc400.weight.data = torch.rand(512,1024) * 2 -1\n",
    "# fc400.bias.data = torch.rand(512) * 2 -1\n",
    "print(fc400.weight.size())\n",
    "# print(fc400.weight)\n",
    "\n",
    "\n",
    "# ufc400 = UnaryLinear(400, 120, 256, fc400.weight, fc400.weight, bias=True)\n",
    "ufc400 = UnaryLinear(1024, 512, 512, model.fc1.weight, model.fc1.bias, bias=True)\n",
    "\n",
    "\n",
    "inVec = (((torch.rand(1024) * 2 - 1))*256).floor()/256\n",
    "# print(inVec)\n",
    "outVec = fc400(inVec)\n",
    "outVec.clamp_(-1.,1.)\n",
    "\n",
    "bsGen = BitStreamGen(inVec, mode=\"Sobol\", bipolar=True)\n",
    "# bsGen = BitStreamGen(inVec,mode=\"Race\")\n",
    "pp = ProgressivePrecision(outVec, auto_print=False,bipolar=True)\n",
    "ipp = ProgressivePrecision(inVec, auto_print=False,bipolar=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(256):\n",
    "        input = bsGen.Gen()\n",
    "#         print(ipp.Monitor(input))\n",
    "        output = ufc400(input)\n",
    "        pp.Monitor(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = bsGen.Gen()\n",
    "# print(ipp.Monitor(input))\n",
    "# output = ufc400(input)\n",
    "# pp.Monitor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.in_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "class UnaryScaledADD(torch.nn.modules.pooling.AvgPool2d):\n",
    "    \"\"\"unary scaled addition\"\"\"\n",
    "    def __init__(self, kernel_size, input_shape, stride=None, padding=0, ceil_mode=False,\n",
    "                 count_include_pad=True, divisor_override=None):\n",
    "        super(UnaryScaledADD, self).__init__(kernel_size, input_shape)\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride or kernel_size\n",
    "        self.padding = padding\n",
    "        self.ceil_mode = ceil_mode\n",
    "        self.count_include_pad = count_include_pad\n",
    "        self.divisor_override = divisor_override\n",
    "        \n",
    "        self.output_shape = list(input_shape)\n",
    "        # data bit width\n",
    "        if stride is None:\n",
    "            if isinstance(kernel_size, int):\n",
    "                self.scale = kernel_size*kernel_size\n",
    "                self.output_shape[2] = int((input_shape[2] + 2 * padding - kernel_size) / kernel_size + 1)\n",
    "                self.output_shape[3] = int((input_shape[3] + 2 * padding - kernel_size) / kernel_size + 1)\n",
    "            elif isinstance(kernel_size, tuple):\n",
    "                self.scale = kernel_size[0]*kernel_size[1]\n",
    "                self.output_shape[2] = int((input_shape[2] + 2 * padding - kernel_size[0]) / stride[0] + 1)\n",
    "                self.output_shape[3] = int((input_shape[3] + 2 * padding - kernel_size[1]) / stride[1] + 1)\n",
    "        else:\n",
    "            # to do\n",
    "            pass\n",
    "        \n",
    "        self.in_accumulator = torch.zeros(self.output_shape)\n",
    "        self.output = torch.zeros(self.output_shape)\n",
    "        \n",
    "        # define the kernel avgpool2d\n",
    "        self.avgpool2d = torch.nn.AvgPool2d(self.kernel_size, \n",
    "                                            stride=self.stride, padding=self.padding, \n",
    "                                            ceil_mode=self.ceil_mode, \n",
    "                                            count_include_pad=self.count_include_pad, \n",
    "                                            divisor_override=self.divisor_override)\n",
    "        \n",
    "    def UnaryScaledADD_forward(self, input):\n",
    "        self.in_accumulator.add_(self.avgpool2d(input))\n",
    "        self.output = torch.ge(self.in_accumulator, 1).type(torch.float)\n",
    "        self.in_accumulator.sub_(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.UnaryScaledADD_forward(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = UnaryScaledADD(2, (1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c(torch.ones([1,1,28,28])/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "class UnaryCompare(torch.nn.modules.Module):\n",
    "    \"\"\"unary comparator\"\"\"\n",
    "    def __init__(self, input_shape):\n",
    "        super(UnaryCompare, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.out_accumulator = torch.zeros([1,input_shape])\n",
    "        self.out_acc_sign = torch.zeros([1,input_shape])\n",
    "        self.output = torch.zeros([1,input_shape])\n",
    "\n",
    "    def UnaryCompare_forward(self, input):\n",
    "        self.out_acc_sign = torch.lt(self.out_accumulator, 0).type(torch.float)\n",
    "        self.output = self.out_acc_sign + (1 - self.out_acc_sign) * input\n",
    "        self.out_accumulator.add_(2 * self.output - 1)\n",
    "        return self.output\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.UnaryCompare_forward(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unaryNet(nn.Module):\n",
    "    def __init__(self, model, prediction):\n",
    "        super(unaryNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.prediction = prediction\n",
    "        self.fc1 = UnaryLinear(32*32, 512, 256, model.fc1.weight, model.fc1.bias)\n",
    "        self.fc1_relu = UnaryCompare(512)\n",
    "        self.fc2 = UnaryLinear(512, 256, 256, model.fc2.weight, model.fc2.bias)\n",
    "        self.fc2_relu = UnaryCompare(256)\n",
    "        self.fc3 = UnaryLinear(256, 10, 256, model.fc3.weight, model.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = self.fc1_relu(self.fc1(x))\n",
    "        x = self.fc2_relu(self.fc2(x))\n",
    "        self.pp = ProgressivePrecision(self.prediction)\n",
    "        return F.log_softmax(self.pp.Monitor(self.fc3(x)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([9]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([5])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([4]) tensor([5])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([3]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([5])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([9]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([4]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([9]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([5])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([9]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([5])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([5])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([2]) tensor([5])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([8])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([9]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([0])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([4]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([4]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([9])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([9]) tensor([2])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([8])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([1]) tensor([4])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([7])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([7]) tensor([6])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([0]) tensor([1])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n",
      "tensor([6]) tensor([3])\n",
      "Has bias.\n",
      "Has bias.\n",
      "Has bias.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d744d5aa92fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbsGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBitStreamGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0muGEMM_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muGEMMnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muGEMM_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-786a9d7d7b1d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgressivePrecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-392e9fbe91b1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnaryKernel_nonscaled_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;31m#         .clamp_(-self.upper_bound, self.upper_bound)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-392e9fbe91b1>\u001b[0m in \u001b[0;36mUnaryKernel_nonscaled_forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_wght_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_wght_idx_inv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_inv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_wght\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_wght_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_wght_idx_inv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        uGEMMnet = unaryNet(model, outputs)\n",
    "        bsGen = BitStreamGen(images)\n",
    "        for i in range(256):\n",
    "            uGEMM_out = uGEMMnet(bsGen.Gen())\n",
    "            _, predicted = torch.max(uGEMM_out.data, 1)\n",
    "            if predicted == labels:\n",
    "                print(\"yes\")\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(predicted, labels)\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 0.000000 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
